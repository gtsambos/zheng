---
title: "8schools"
author: "Gtsambos"
date: "07/01/2019"
header-includes:
   - \usepackage{bbm}
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The stan code

Details of the model are encoded in a a Stan file called `8schools.stan`.

Let's analyse this code bit-by-bit.

#### Data

This chunk creates the variables that represent constants in the model:

 - $J$ is the number of schools.
 - $y_j$ is the estimated increase in SAT scores at the $j$th school, for $j = 1,\ldots, J$. From here on, I will refer to this as the 'treatment effect'.
 - $\sigma_j$ is the standard error of the estimated treatment effect, for $j = 1,\ldots, J$.
 
 Note that $\sigma_j$ is not considered a `parameter` as we are assuming that there are known constant values for $\sigma_j$ in this analysis (see the tect_.)

```
data {
  int<lower=0> J;         // number of schools 
  real y[J];              // estimated treatment effects
  real<lower=0> sigma[J]; // standard error of effect estimates 
}
```
#### Parameters

This chunk defines the unknown variables in this analysis that we wish the estimate. These are:

 - $\mu \in \mathbb{R}$, the treatment effect across schools
 - $\tau$, the standard deviation in treatment effects across schools
 - $\eta_j$, the amount by which the $j$th school's treatment effect differs from $mu$.

```
parameters {
  real mu;                // population treatment effect
  real<lower=0> tau;      // standard deviation in treatment effects
  vector[J] eta;          // unscaled deviation from mu by school
}
```

#### Transformed parameters

This chunk defines parameters which can be represented as a function of variables that have already been defined in the previous chunks.

In particular, the treatment effect for the $j$th school is
\[
\theta_j = \mu + \tau \cdot \eta_j \quad \text{ for } j = 1,\ldots, J.\tag{1}
\]

Note that we could directly define $\theta_j$ in the `parameters` block, but this 'uncentred' implementation can introduce bias into the Markov chain sampling process. (See [here](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html) for more.)

```
transformed parameters {
  vector[J] theta = mu + tau * eta;        // school treatment effects
}
```

#### Model

This chunk defines the distributions of the parameters in the model that were in `parameters`.

 - As per eqn (5.14) in BDA3, the school-specific effect estimates $\theta_j$ are assumed to be independent draws from a normal distribution with mean $\mu$ and standard deviation $\tau$:
\[
p(\theta_1, \ldots, \theta_J \mid \mu, \tau)
= \prod_{j=1}^J p(\theta_1 \mid \mu, \tau) 
= \prod_{j=1}^J N \left( \theta_j \mid \mu, \tau^2 \right)\tag{BDA5.14}
\]
or equivalently,
\[
\log p(\theta_1, \ldots, \theta_J \mid \mu, \tau)
 = \sum_{j=1}^J \log p(\theta_1 \mid \mu, \tau) 
 = \sum_{j=1}^J \log N \left( \theta_j \mid \mu, \tau^2 \right)
\]
where by $N(a,b)$ I mean the [pdf of a normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean $a$ and variance $b$.
However, from (1) and (BDA.14), each $\theta_j$ is simply a transformed version of a standard normal variable $\eta_j$:

\[
\log p(\eta_1, \ldots, \eta_J \mid \mu, \tau)
 = \sum_{j=1}^J \log N\left( 0, 1\right).
\]

 - As per eqn (5.11) in BDA3, conditional on the school-specific effects $\theta_j$, the observed effects $y_{ij}$ for the $i$th student in school $j$ have the following distribution:
 \[
 y_{ij} \mid \theta_j \sim N(\theta_j, \sigma^2).\tag{BDA5.11}
 \]
 Since each student's score is presumed to be independent after conditioning on their school, the log likelihood of the observed data is
 \[
 \log p(y_1, \ldots, y_J \mid \theta_1, \ldots, \theta_J, \sigma_1, \ldots, \sigma_j) = \sum_{i = 1}^{n_j} \sum_{j=1}^J \log N(\theta_j, \sigma_j^2).
 \]
 
Note that these statements are coded in a more concise, vectorised form.
 
  - The prior distribution for $\mu$ is not explicitly coded: it takes an improper uniform distribution given $\tau$ (see BDA5.15), which is the default for unspecified parameters in Stan.
  

 
 
 


```
model {
  target += normal_lpdf(eta | 0, 1);       // prior log-density
  target += normal_lpdf(y | theta, sigma); // log-likelihood
}
```

## The R code

First, we need to load RStan.

```{r loadpackages,echo=FALSE}
library(rstan)
```

Next, we'll set values for each of the known quantities defined in the `data` chunk of the Stan script. As per Table 5.2, we have
\begin{align*}
J &= 8 \text{ schools }\\[1mm]
(\bar{y_1}, \ldots, \bar{y_8} ) 
&= (28, \ldots, 12)\\[1mm]
(\sigma_1, \ldots, \sigma_8) 
&= (15, \ldots, 18).
\end{align*}

```{r dataentry}
schools_dat <- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
```
Next let's fit the model.

```{r modelfit,echo=FALSE}
fit <- stan(file = '8schools.stan', data = schools_dat)
```

We now have an `stanfit` object. This summary of the estimated model is largely similar to the one obtained by the authors, and can be interpreted similarly. Type in `help(stanfit-class)` to see other things in this object.

```{r summaryresults}
print(fit)
```
